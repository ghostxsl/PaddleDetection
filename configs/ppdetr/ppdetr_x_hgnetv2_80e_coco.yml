_BASE_: [
  '../datasets/coco_detection.yml',
  '../runtime.yml',
  '_base_/optimizer_80e.yml',
  '_base_/ppdetr_r50vd.yml',
  '_base_/ppdetr_reader.yml',
]

weights: output/ppdetr_l_hgnetv2_80e_coco/model_final
find_unused_parameters: True
log_iter: 200

pretrain_weights: http://10.21.226.186:8787/workspace/backup/ppdetr/hgnet_v2_x_pretrained.pdparams

DETR:
  backbone: PPHGNetV2
  neck: HybridEncoder
  transformer: PPDETRTransformer
  detr_head: DINOHead
  post_process: DETRPostProcess

PPHGNetV2:
  stem_channels: [32, 32, 64]
  stage_config: {
        # in_channels, mid_channels, out_channels, blocks, downsample, dw_block, dw_kernel_size, act
        "stage1": [64, 64, 128, 1, False, False, 3, "ReLU"],
        "stage2": [128, 128, 512, 2, True, False, 3, "ReLU"],
        "stage3": [512, 256, 1024, 5, True, True, 5, "ReLU"],
        "stage4": [1024, 512, 2048, 2, True, True, 5, "ReLU"],
  }
  depth_mult: 1.0
  width_mult: 1.0
  layer_num: 6
  dw_act: [False, True]
  return_idx: [1, 2, 3]
  freeze_stem_only: True
  freeze_at: 0
  freeze_norm: True
  lr_mult_list: [0.05, 0.05, 0.05, 0.05]

HybridEncoder:
  depth_mult: 1.0
  hidden_dim: 384
  use_encoder_idx: [2]
  num_encoder_layers: 1
  encoder_layer:
    name: TransformerLayer
    d_model: 384
    nhead: 8
    dim_feedforward: 2048
    dropout: 0.
    activation: 'gelu'
  expansion: 1.0
